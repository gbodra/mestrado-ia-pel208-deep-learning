{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "DeepLearning.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "authorship_tag": "ABX9TyOQjRXSq9lksqMPtCRIFSGi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/gbodra/mestrado-ia-pel208-deep-learning/blob/main/DeepLearning.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "qSU_KQ2AqXlb"
      },
      "source": [
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "import numpy as np\n",
        "import time\n",
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "(train_x, train_y), (test_x, test_y) = keras.datasets.mnist.load_data()\n",
        "train_x = train_x / 255.0\n",
        "test_x = test_x / 255.0\n",
        "\n",
        "train_x = tf.expand_dims(train_x, 3)\n",
        "test_x = tf.expand_dims(test_x, 3)\n",
        "\n",
        "val_x = train_x[:5000]\n",
        "val_y = train_y[:5000]"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9AuF9QldngO3"
      },
      "source": [
        "lenet_5_model = keras.models.Sequential([\n",
        "    keras.layers.Conv2D(6, kernel_size=5, strides=1,  activation='tanh', input_shape=train_x[0].shape, padding='same'),\n",
        "    keras.layers.AveragePooling2D(),\n",
        "    keras.layers.Conv2D(16, kernel_size=5, strides=1, activation='tanh', padding='valid'),\n",
        "    keras.layers.AveragePooling2D(),\n",
        "    keras.layers.Flatten(),\n",
        "    keras.layers.Dense(120, activation='tanh'),\n",
        "    keras.layers.Dense(84, activation='tanh'),\n",
        "    keras.layers.Dense(10, activation='softmax')\n",
        "])"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xlSiJpwwnpAj"
      },
      "source": [
        "lenet_5_model.compile(optimizer='adam', loss=keras.losses.sparse_categorical_crossentropy, metrics=['accuracy'])"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "1uy8VcwXn2mS",
        "outputId": "d9d0aa10-d011-490c-b9e2-6b9c664fa4f2"
      },
      "source": [
        "start_time = datetime.timestamp(datetime.now())\n",
        "lenet_5_model.fit(train_x, train_y, epochs=100, validation_data=(val_x, val_y))\n",
        "end_time = datetime.timestamp(datetime.now())\n",
        "elapsed = end_time - start_time\n",
        "elapsed"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.2236 - accuracy: 0.9337 - val_loss: 0.0874 - val_accuracy: 0.9768\n",
            "Epoch 2/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0850 - accuracy: 0.9744 - val_loss: 0.0522 - val_accuracy: 0.9836\n",
            "Epoch 3/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0581 - accuracy: 0.9823 - val_loss: 0.0355 - val_accuracy: 0.9910\n",
            "Epoch 4/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0433 - accuracy: 0.9867 - val_loss: 0.0309 - val_accuracy: 0.9910\n",
            "Epoch 5/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0363 - accuracy: 0.9882 - val_loss: 0.0273 - val_accuracy: 0.9916\n",
            "Epoch 6/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0286 - accuracy: 0.9908 - val_loss: 0.0262 - val_accuracy: 0.9914\n",
            "Epoch 7/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0238 - accuracy: 0.9925 - val_loss: 0.0185 - val_accuracy: 0.9934\n",
            "Epoch 8/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0211 - accuracy: 0.9927 - val_loss: 0.0189 - val_accuracy: 0.9942\n",
            "Epoch 9/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0191 - accuracy: 0.9938 - val_loss: 0.0109 - val_accuracy: 0.9962\n",
            "Epoch 10/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0145 - accuracy: 0.9954 - val_loss: 0.0095 - val_accuracy: 0.9960\n",
            "Epoch 11/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0146 - accuracy: 0.9953 - val_loss: 0.0108 - val_accuracy: 0.9964\n",
            "Epoch 12/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0132 - accuracy: 0.9958 - val_loss: 0.0106 - val_accuracy: 0.9968\n",
            "Epoch 13/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0126 - accuracy: 0.9959 - val_loss: 0.0089 - val_accuracy: 0.9972\n",
            "Epoch 14/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0103 - accuracy: 0.9967 - val_loss: 0.0062 - val_accuracy: 0.9980\n",
            "Epoch 15/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0114 - accuracy: 0.9962 - val_loss: 0.0091 - val_accuracy: 0.9970\n",
            "Epoch 16/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0090 - accuracy: 0.9971 - val_loss: 0.0060 - val_accuracy: 0.9982\n",
            "Epoch 17/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0104 - accuracy: 0.9963 - val_loss: 0.0047 - val_accuracy: 0.9986\n",
            "Epoch 18/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0088 - accuracy: 0.9971 - val_loss: 0.0042 - val_accuracy: 0.9984\n",
            "Epoch 19/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0032 - val_accuracy: 0.9992\n",
            "Epoch 20/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0075 - accuracy: 0.9975 - val_loss: 0.0043 - val_accuracy: 0.9984\n",
            "Epoch 21/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0094 - accuracy: 0.9969 - val_loss: 0.0064 - val_accuracy: 0.9974\n",
            "Epoch 22/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0019 - val_accuracy: 0.9996\n",
            "Epoch 23/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0043 - val_accuracy: 0.9982\n",
            "Epoch 24/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0058 - accuracy: 0.9980 - val_loss: 0.0038 - val_accuracy: 0.9984\n",
            "Epoch 25/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0060 - accuracy: 0.9980 - val_loss: 0.0042 - val_accuracy: 0.9986\n",
            "Epoch 26/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0077 - accuracy: 0.9974 - val_loss: 0.0034 - val_accuracy: 0.9988\n",
            "Epoch 27/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0059 - accuracy: 0.9980 - val_loss: 0.0071 - val_accuracy: 0.9976\n",
            "Epoch 28/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.0034 - val_accuracy: 0.9990\n",
            "Epoch 29/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0063 - accuracy: 0.9979 - val_loss: 0.0062 - val_accuracy: 0.9976\n",
            "Epoch 30/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0056 - accuracy: 0.9981 - val_loss: 0.0034 - val_accuracy: 0.9988\n",
            "Epoch 31/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0052 - accuracy: 0.9983 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
            "Epoch 32/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0062 - accuracy: 0.9981 - val_loss: 0.0043 - val_accuracy: 0.9978\n",
            "Epoch 33/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0034 - val_accuracy: 0.9988\n",
            "Epoch 34/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0046 - accuracy: 0.9985 - val_loss: 0.0055 - val_accuracy: 0.9978\n",
            "Epoch 35/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0047 - accuracy: 0.9984 - val_loss: 0.0050 - val_accuracy: 0.9986\n",
            "Epoch 36/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0039 - accuracy: 0.9988 - val_loss: 0.0083 - val_accuracy: 0.9974\n",
            "Epoch 37/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0057 - accuracy: 0.9981 - val_loss: 0.0043 - val_accuracy: 0.9986\n",
            "Epoch 38/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0043 - accuracy: 0.9985 - val_loss: 0.0022 - val_accuracy: 0.9990\n",
            "Epoch 39/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0024 - val_accuracy: 0.9994\n",
            "Epoch 40/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0051 - accuracy: 0.9982 - val_loss: 0.0026 - val_accuracy: 0.9990\n",
            "Epoch 41/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0048 - accuracy: 0.9984 - val_loss: 0.0053 - val_accuracy: 0.9984\n",
            "Epoch 42/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0130 - val_accuracy: 0.9956\n",
            "Epoch 43/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0055 - accuracy: 0.9982 - val_loss: 0.0072 - val_accuracy: 0.9982\n",
            "Epoch 44/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0027 - val_accuracy: 0.9990\n",
            "Epoch 45/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0022 - val_accuracy: 0.9994\n",
            "Epoch 46/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0048 - val_accuracy: 0.9986\n",
            "Epoch 47/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0011 - val_accuracy: 0.9996\n",
            "Epoch 48/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 0.0055 - val_accuracy: 0.9974\n",
            "Epoch 49/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0046 - accuracy: 0.9984 - val_loss: 0.0042 - val_accuracy: 0.9986\n",
            "Epoch 50/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0036 - accuracy: 0.9987 - val_loss: 0.0014 - val_accuracy: 0.9998\n",
            "Epoch 51/100\n",
            "1875/1875 [==============================] - 5s 2ms/step - loss: 0.0038 - accuracy: 0.9987 - val_loss: 0.0013 - val_accuracy: 0.9996\n",
            "Epoch 52/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0044 - accuracy: 0.9986 - val_loss: 0.0028 - val_accuracy: 0.9988\n",
            "Epoch 53/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0047 - accuracy: 0.9983 - val_loss: 0.0015 - val_accuracy: 0.9996\n",
            "Epoch 54/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0020 - accuracy: 0.9993 - val_loss: 0.0029 - val_accuracy: 0.9986\n",
            "Epoch 55/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0024 - val_accuracy: 0.9988\n",
            "Epoch 56/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0040 - accuracy: 0.9984 - val_loss: 0.0016 - val_accuracy: 0.9994\n",
            "Epoch 57/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0044 - accuracy: 0.9985 - val_loss: 0.0020 - val_accuracy: 0.9996\n",
            "Epoch 58/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0025 - val_accuracy: 0.9992\n",
            "Epoch 59/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0045 - accuracy: 0.9985 - val_loss: 0.0025 - val_accuracy: 0.9992\n",
            "Epoch 60/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 6.7820e-04 - val_accuracy: 1.0000\n",
            "Epoch 61/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0025 - accuracy: 0.9992 - val_loss: 0.0012 - val_accuracy: 0.9994\n",
            "Epoch 62/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0037 - accuracy: 0.9987 - val_loss: 0.0024 - val_accuracy: 0.9994\n",
            "Epoch 63/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0044 - accuracy: 0.9984 - val_loss: 0.0019 - val_accuracy: 0.9994\n",
            "Epoch 64/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0042 - accuracy: 0.9986 - val_loss: 0.0036 - val_accuracy: 0.9984\n",
            "Epoch 65/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0015 - val_accuracy: 0.9998\n",
            "Epoch 66/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0019 - accuracy: 0.9994 - val_loss: 0.0039 - val_accuracy: 0.9986\n",
            "Epoch 67/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0050 - accuracy: 0.9983 - val_loss: 0.0022 - val_accuracy: 0.9994\n",
            "Epoch 68/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0018 - val_accuracy: 0.9994\n",
            "Epoch 69/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9990 - val_loss: 0.0016 - val_accuracy: 0.9992\n",
            "Epoch 70/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0037 - accuracy: 0.9985 - val_loss: 0.0027 - val_accuracy: 0.9994\n",
            "Epoch 71/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0023 - accuracy: 0.9991 - val_loss: 0.0049 - val_accuracy: 0.9984\n",
            "Epoch 72/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0050 - accuracy: 0.9984 - val_loss: 0.0072 - val_accuracy: 0.9980\n",
            "Epoch 73/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9988 - val_loss: 0.0050 - val_accuracy: 0.9988\n",
            "Epoch 74/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0040 - accuracy: 0.9987 - val_loss: 0.0011 - val_accuracy: 0.9998\n",
            "Epoch 75/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0023 - accuracy: 0.9993 - val_loss: 0.0015 - val_accuracy: 0.9996\n",
            "Epoch 76/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0030 - accuracy: 0.9991 - val_loss: 0.0030 - val_accuracy: 0.9990\n",
            "Epoch 77/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0022 - val_accuracy: 0.9988\n",
            "Epoch 78/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0041 - accuracy: 0.9988 - val_loss: 0.0024 - val_accuracy: 0.9992\n",
            "Epoch 79/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0029 - accuracy: 0.9991 - val_loss: 0.0035 - val_accuracy: 0.9990\n",
            "Epoch 80/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0049 - accuracy: 0.9985 - val_loss: 0.0014 - val_accuracy: 0.9996\n",
            "Epoch 81/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0029 - val_accuracy: 0.9988\n",
            "Epoch 82/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0036 - accuracy: 0.9989 - val_loss: 4.9002e-04 - val_accuracy: 1.0000\n",
            "Epoch 83/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0029 - accuracy: 0.9992 - val_loss: 0.0018 - val_accuracy: 0.9992\n",
            "Epoch 84/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0029 - accuracy: 0.9988 - val_loss: 0.0040 - val_accuracy: 0.9988\n",
            "Epoch 85/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0024 - accuracy: 0.9992 - val_loss: 0.0014 - val_accuracy: 0.9996\n",
            "Epoch 86/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9987 - val_loss: 0.0037 - val_accuracy: 0.9990\n",
            "Epoch 87/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0036 - accuracy: 0.9988 - val_loss: 0.0022 - val_accuracy: 0.9990\n",
            "Epoch 88/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0032 - val_accuracy: 0.9986\n",
            "Epoch 89/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0045 - val_accuracy: 0.9986\n",
            "Epoch 90/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 0.0016 - val_accuracy: 0.9992\n",
            "Epoch 91/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0027 - accuracy: 0.9991 - val_loss: 0.0018 - val_accuracy: 0.9994\n",
            "Epoch 92/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0020 - accuracy: 0.9992 - val_loss: 0.0017 - val_accuracy: 0.9994\n",
            "Epoch 93/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0033 - accuracy: 0.9989 - val_loss: 4.4079e-04 - val_accuracy: 1.0000\n",
            "Epoch 94/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0031 - accuracy: 0.9991 - val_loss: 0.0024 - val_accuracy: 0.9988\n",
            "Epoch 95/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0023 - accuracy: 0.9994 - val_loss: 0.0042 - val_accuracy: 0.9992\n",
            "Epoch 96/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0018 - accuracy: 0.9994 - val_loss: 0.0012 - val_accuracy: 0.9994\n",
            "Epoch 97/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0028 - accuracy: 0.9991 - val_loss: 0.0021 - val_accuracy: 0.9992\n",
            "Epoch 98/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0028 - accuracy: 0.9992 - val_loss: 4.1247e-04 - val_accuracy: 1.0000\n",
            "Epoch 99/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0034 - accuracy: 0.9989 - val_loss: 0.0030 - val_accuracy: 0.9986\n",
            "Epoch 100/100\n",
            "1875/1875 [==============================] - 4s 2ms/step - loss: 0.0024 - accuracy: 0.9991 - val_loss: 2.0286e-04 - val_accuracy: 1.0000\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "426.4237208366394"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F_OGEkeUn6E3",
        "outputId": "80e6ddb3-fbd4-4d67-d63a-aba5564246b8"
      },
      "source": [
        "lenet_5_model.evaluate(test_x, test_y)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "313/313 [==============================] - 1s 2ms/step - loss: 0.0657 - accuracy: 0.9885\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[0.06565392762422562, 0.9884999990463257]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "j7oHjijBSNPF",
        "outputId": "a22a5245-de2f-4246-ef8a-8e5bb33f2df6"
      },
      "source": [
        "!pip install git+git://github.com/albertbup/deep-belief-network.git@master_gpu"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting git+git://github.com/albertbup/deep-belief-network.git@master_gpu\n",
            "  Cloning git://github.com/albertbup/deep-belief-network.git (to revision master_gpu) to /tmp/pip-req-build-p8u6rrf4\n",
            "  Running command git clone -q git://github.com/albertbup/deep-belief-network.git /tmp/pip-req-build-p8u6rrf4\n",
            "  Running command git checkout -b master_gpu --track origin/master_gpu\n",
            "  Switched to a new branch 'master_gpu'\n",
            "  Branch 'master_gpu' set up to track remote branch 'master_gpu' from 'origin'.\n",
            "Requirement already satisfied: numpy>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.18.5)\n",
            "Requirement already satisfied: scipy>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (1.4.1)\n",
            "Requirement already satisfied: scikit-learn>=0.18.1 in /usr/local/lib/python3.6/dist-packages (from deep-belief-network==1.0.3) (0.22.2.post1)\n",
            "Collecting tensorflow-gpu>=1.5.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/18/99/ac32fd13d56e40d4c3e6150030132519997c0bb1f06f448d970e81b177e5/tensorflow_gpu-2.3.1-cp36-cp36m-manylinux2010_x86_64.whl (320.4MB)\n",
            "\u001b[K     |████████████████████████████████| 320.4MB 57kB/s \n",
            "\u001b[?25hRequirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.6/dist-packages (from scikit-learn>=0.18.1->deep-belief-network==1.0.3) (0.17.0)\n",
            "Requirement already satisfied: grpcio>=1.8.6 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.34.0)\n",
            "Requirement already satisfied: termcolor>=1.1.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.1.0)\n",
            "Requirement already satisfied: wrapt>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.12.1)\n",
            "Requirement already satisfied: wheel>=0.26 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.36.1)\n",
            "Requirement already satisfied: six>=1.12.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.15.0)\n",
            "Requirement already satisfied: absl-py>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.10.0)\n",
            "Requirement already satisfied: protobuf>=3.9.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.12.4)\n",
            "Requirement already satisfied: h5py<2.11.0,>=2.10.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (2.10.0)\n",
            "Requirement already satisfied: tensorflow-estimator<2.4.0,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (2.3.0)\n",
            "Requirement already satisfied: opt-einsum>=2.3.2 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.3.0)\n",
            "Requirement already satisfied: astunparse==1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.6.3)\n",
            "Requirement already satisfied: gast==0.3.3 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.3.3)\n",
            "Requirement already satisfied: tensorboard<3,>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (2.3.0)\n",
            "Requirement already satisfied: keras-preprocessing<1.2,>=1.1.1 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.1.2)\n",
            "Requirement already satisfied: google-pasta>=0.1.8 in /usr/local/lib/python3.6/dist-packages (from tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.2.0)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.6/dist-packages (from protobuf>=3.9.2->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (50.3.2)\n",
            "Requirement already satisfied: werkzeug>=0.11.15 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.0.1)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.3.3)\n",
            "Requirement already satisfied: google-auth<2,>=1.6.3 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.17.2)\n",
            "Requirement already satisfied: google-auth-oauthlib<0.5,>=0.4.1 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.4.2)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (2.23.0)\n",
            "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /usr/local/lib/python3.6/dist-packages (from tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.7.0)\n",
            "Requirement already satisfied: importlib-metadata; python_version < \"3.8\" in /usr/local/lib/python3.6/dist-packages (from markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.1.1)\n",
            "Requirement already satisfied: cachetools<5.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (4.1.1)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4; python_version >= \"3\" in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (4.6)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.6/dist-packages (from google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.2.8)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.6/dist-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.3.0)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (1.24.3)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.0.4)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests<3,>=2.21.0->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (2020.12.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.6/dist-packages (from importlib-metadata; python_version < \"3.8\"->markdown>=2.6.8->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.4.0)\n",
            "Requirement already satisfied: pyasn1>=0.1.3 in /usr/local/lib/python3.6/dist-packages (from rsa<5,>=3.1.4; python_version >= \"3\"->google-auth<2,>=1.6.3->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (0.4.8)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.6/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<3,>=2.3.0->tensorflow-gpu>=1.5.0->deep-belief-network==1.0.3) (3.1.0)\n",
            "Building wheels for collected packages: deep-belief-network\n",
            "  Building wheel for deep-belief-network (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deep-belief-network: filename=deep_belief_network-1.0.3-cp36-none-any.whl size=13464 sha256=3bb7288b397d80686419bb4c18c8312bc02d63f176c090b94c52c56bb0d7112d\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-2ovwl8ae/wheels/fe/5a/c7/18fc337ca5590a8b7c74c17ddba7a63935ce7a5bafb6cdec02\n",
            "Successfully built deep-belief-network\n",
            "Installing collected packages: tensorflow-gpu, deep-belief-network\n",
            "Successfully installed deep-belief-network-1.0.3 tensorflow-gpu-2.3.1\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "tensorflow"
                ]
              }
            }
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8QUTdQZkSVRB",
        "outputId": "d299afd4-b069-43fb-82e8-1eb184dccb72"
      },
      "source": [
        "import numpy as np\n",
        "\n",
        "np.random.seed(0)\n",
        "from sklearn.datasets import load_digits\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics.classification import accuracy_score\n",
        "from datetime import datetime\n",
        "\n",
        "start_time = datetime.timestamp(datetime.now())\n",
        "\n",
        "# from dbn.tensorflow import SupervisedDBNClassification\n",
        "from dbn import SupervisedDBNClassification\n",
        "# use \"from dbn import SupervisedDBNClassification\" for computations on CPU with numpy\n",
        "\n",
        "# Loading dataset\n",
        "digits = load_digits()\n",
        "X, Y = digits.data, digits.target\n",
        "\n",
        "# Data scaling\n",
        "X = (X / 16).astype(np.float32)\n",
        "\n",
        "# Splitting data\n",
        "X_train, X_test, Y_train, Y_test = train_test_split(X, Y, test_size=0.2, random_state=0)\n",
        "\n",
        "# Training\n",
        "classifier = SupervisedDBNClassification(hidden_layers_structure=[256, 256],\n",
        "                                         learning_rate_rbm=0.05,\n",
        "                                         learning_rate=0.1,\n",
        "                                         n_epochs_rbm=10,\n",
        "                                         n_iter_backprop=100,\n",
        "                                         batch_size=32,\n",
        "                                         activation_function='relu',\n",
        "                                         dropout_p=0.2)\n",
        "classifier.fit(X_train, Y_train)\n",
        "\n",
        "# Save the model\n",
        "classifier.save('model.pkl')\n",
        "\n",
        "# Restore it\n",
        "classifier = SupervisedDBNClassification.load('model.pkl')\n",
        "\n",
        "# Test\n",
        "Y_pred = classifier.predict(X_test)\n",
        "print('Done.\\nAccuracy: %f' % accuracy_score(Y_test, Y_pred))\n",
        "\n",
        "end_time = datetime.timestamp(datetime.now())\n",
        "elapsed = end_time - start_time\n",
        "elapsed"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[START] Pre-training step:\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 2.710084\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 1.796745\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.434709\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 1.201816\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 1.108185\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.992179\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.916095\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.919944\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.853218\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.850179\n",
            ">> Epoch 1 finished \tRBM Reconstruction error 2.870218\n",
            ">> Epoch 2 finished \tRBM Reconstruction error 1.499493\n",
            ">> Epoch 3 finished \tRBM Reconstruction error 1.111499\n",
            ">> Epoch 4 finished \tRBM Reconstruction error 0.950356\n",
            ">> Epoch 5 finished \tRBM Reconstruction error 0.802263\n",
            ">> Epoch 6 finished \tRBM Reconstruction error 0.677447\n",
            ">> Epoch 7 finished \tRBM Reconstruction error 0.610802\n",
            ">> Epoch 8 finished \tRBM Reconstruction error 0.537196\n",
            ">> Epoch 9 finished \tRBM Reconstruction error 0.486881\n",
            ">> Epoch 10 finished \tRBM Reconstruction error 0.444573\n",
            "[END] Pre-training step\n",
            "[START] Fine tuning step:\n",
            ">> Epoch 1 finished \tANN training loss 19.869630\n",
            ">> Epoch 2 finished \tANN training loss 12.792749\n",
            ">> Epoch 3 finished \tANN training loss 8.908449\n",
            ">> Epoch 4 finished \tANN training loss 7.460814\n",
            ">> Epoch 5 finished \tANN training loss 6.192292\n",
            ">> Epoch 6 finished \tANN training loss 5.865656\n",
            ">> Epoch 7 finished \tANN training loss 5.090892\n",
            ">> Epoch 8 finished \tANN training loss 4.877600\n",
            ">> Epoch 9 finished \tANN training loss 4.831434\n",
            ">> Epoch 10 finished \tANN training loss 4.286283\n",
            ">> Epoch 11 finished \tANN training loss 4.344606\n",
            ">> Epoch 12 finished \tANN training loss 4.509297\n",
            ">> Epoch 13 finished \tANN training loss 4.024831\n",
            ">> Epoch 14 finished \tANN training loss 3.773606\n",
            ">> Epoch 15 finished \tANN training loss 3.783466\n",
            ">> Epoch 16 finished \tANN training loss 4.080954\n",
            ">> Epoch 17 finished \tANN training loss 3.581892\n",
            ">> Epoch 18 finished \tANN training loss 3.511160\n",
            ">> Epoch 19 finished \tANN training loss 3.623912\n",
            ">> Epoch 20 finished \tANN training loss 3.686783\n",
            ">> Epoch 21 finished \tANN training loss 3.803639\n",
            ">> Epoch 22 finished \tANN training loss 3.493755\n",
            ">> Epoch 23 finished \tANN training loss 3.572870\n",
            ">> Epoch 24 finished \tANN training loss 3.153068\n",
            ">> Epoch 25 finished \tANN training loss 3.043105\n",
            ">> Epoch 26 finished \tANN training loss 3.274945\n",
            ">> Epoch 27 finished \tANN training loss 3.182741\n",
            ">> Epoch 28 finished \tANN training loss 3.221883\n",
            ">> Epoch 29 finished \tANN training loss 3.356848\n",
            ">> Epoch 30 finished \tANN training loss 3.113141\n",
            ">> Epoch 31 finished \tANN training loss 3.184529\n",
            ">> Epoch 32 finished \tANN training loss 3.146910\n",
            ">> Epoch 33 finished \tANN training loss 3.310838\n",
            ">> Epoch 34 finished \tANN training loss 2.887838\n",
            ">> Epoch 35 finished \tANN training loss 3.059343\n",
            ">> Epoch 36 finished \tANN training loss 2.906240\n",
            ">> Epoch 37 finished \tANN training loss 2.834703\n",
            ">> Epoch 38 finished \tANN training loss 3.264557\n",
            ">> Epoch 39 finished \tANN training loss 2.925966\n",
            ">> Epoch 40 finished \tANN training loss 2.910765\n",
            ">> Epoch 41 finished \tANN training loss 2.883379\n",
            ">> Epoch 42 finished \tANN training loss 3.056049\n",
            ">> Epoch 43 finished \tANN training loss 2.872458\n",
            ">> Epoch 44 finished \tANN training loss 2.859403\n",
            ">> Epoch 45 finished \tANN training loss 2.552211\n",
            ">> Epoch 46 finished \tANN training loss 2.754830\n",
            ">> Epoch 47 finished \tANN training loss 2.714554\n",
            ">> Epoch 48 finished \tANN training loss 2.881486\n",
            ">> Epoch 49 finished \tANN training loss 2.743746\n",
            ">> Epoch 50 finished \tANN training loss 2.744915\n",
            ">> Epoch 51 finished \tANN training loss 2.370393\n",
            ">> Epoch 52 finished \tANN training loss 2.890171\n",
            ">> Epoch 53 finished \tANN training loss 2.726243\n",
            ">> Epoch 54 finished \tANN training loss 2.376481\n",
            ">> Epoch 55 finished \tANN training loss 2.621030\n",
            ">> Epoch 56 finished \tANN training loss 2.778963\n",
            ">> Epoch 57 finished \tANN training loss 2.572508\n",
            ">> Epoch 58 finished \tANN training loss 2.344527\n",
            ">> Epoch 59 finished \tANN training loss 2.325595\n",
            ">> Epoch 60 finished \tANN training loss 2.599043\n",
            ">> Epoch 61 finished \tANN training loss 2.217979\n",
            ">> Epoch 62 finished \tANN training loss 2.404117\n",
            ">> Epoch 63 finished \tANN training loss 2.495050\n",
            ">> Epoch 64 finished \tANN training loss 2.509933\n",
            ">> Epoch 65 finished \tANN training loss 2.583859\n",
            ">> Epoch 66 finished \tANN training loss 2.494227\n",
            ">> Epoch 67 finished \tANN training loss 2.510021\n",
            ">> Epoch 68 finished \tANN training loss 2.343200\n",
            ">> Epoch 69 finished \tANN training loss 2.315084\n",
            ">> Epoch 70 finished \tANN training loss 2.191493\n",
            ">> Epoch 71 finished \tANN training loss 2.284833\n",
            ">> Epoch 72 finished \tANN training loss 2.694688\n",
            ">> Epoch 73 finished \tANN training loss 2.470089\n",
            ">> Epoch 74 finished \tANN training loss 2.447035\n",
            ">> Epoch 75 finished \tANN training loss 2.306036\n",
            ">> Epoch 76 finished \tANN training loss 2.226699\n",
            ">> Epoch 77 finished \tANN training loss 2.563061\n",
            ">> Epoch 78 finished \tANN training loss 2.467483\n",
            ">> Epoch 79 finished \tANN training loss 2.299790\n",
            ">> Epoch 80 finished \tANN training loss 2.158963\n",
            ">> Epoch 81 finished \tANN training loss 2.185666\n",
            ">> Epoch 82 finished \tANN training loss 2.453799\n",
            ">> Epoch 83 finished \tANN training loss 2.299263\n",
            ">> Epoch 84 finished \tANN training loss 2.248463\n",
            ">> Epoch 85 finished \tANN training loss 2.226679\n",
            ">> Epoch 86 finished \tANN training loss 2.111937\n",
            ">> Epoch 87 finished \tANN training loss 2.283832\n",
            ">> Epoch 88 finished \tANN training loss 2.224461\n",
            ">> Epoch 89 finished \tANN training loss 2.136843\n",
            ">> Epoch 90 finished \tANN training loss 2.137479\n",
            ">> Epoch 91 finished \tANN training loss 2.021518\n",
            ">> Epoch 92 finished \tANN training loss 2.163964\n",
            ">> Epoch 93 finished \tANN training loss 2.219556\n",
            ">> Epoch 94 finished \tANN training loss 2.130565\n",
            ">> Epoch 95 finished \tANN training loss 2.073451\n",
            ">> Epoch 96 finished \tANN training loss 2.510411\n",
            ">> Epoch 97 finished \tANN training loss 2.303484\n",
            ">> Epoch 98 finished \tANN training loss 2.318974\n",
            ">> Epoch 99 finished \tANN training loss 2.127533\n",
            ">> Epoch 100 finished \tANN training loss 2.222716\n",
            "[END] Fine tuning step\n",
            "Done.\n",
            "Accuracy: 0.980556\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "103.4248399734497"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DJ17s8saMwzL"
      },
      "source": [
        "# Análise dos resultados\n",
        "## LeNet-5\n",
        "\n",
        "\n",
        "|Epochs\t| Tempo de treinamento | Perda | Acuracidade | GPU |\n",
        "|-------|----------------------|-------|-------------|-----|\n",
        "| 5     | 206s                 | 0.0567| 0.9810      | Não |\n",
        "| 5     | 21s                  | 0.0392| 0.9876      | Sim |\n",
        "| 10    | 416s                 | 0.0497| 0.9855      | Não |\n",
        "| 50    | 212s                 | 0.0626| 0.9859      | Sim |\n",
        "| 100   | 426s                 | 0.0657| 0.9885      | Sim |\n",
        "\n",
        "## DBN\n",
        "\n",
        "|Epochs RBM | Epochs ANN | Tempo de Treinamento | Perda | Acuracidade |\n",
        "|-----------|------------|----------------------|-------|-------------|\n",
        "| 20        | 100        | 103s                 | 2.2227| 0.9805      |\n",
        "\n",
        "## Conclusão\n",
        "\n",
        "Os tempos de execução são muito próximos quando são utilizadas GPUs para realizar o processamento, porém a Lenet-5 produz resultados mais consistentes em menos iterações."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VV8cckDzO1B1"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}